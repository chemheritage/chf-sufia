# See http://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
#
#
<% if CHF::Env.production? %>
# production robots.txt

# We are blocking all robots from /downloads, becuase we have a hypothesis
# that slow proxied-from-fedora /downloads requests from robots are part
# of what's causing problems for our apache. This does mean robots won't
# index PDF originals.
User-agent: *
Disallow: /downloads/
Crawl-delay: 5

User-agent: MauiBot 
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: centurybot9@gmail.com
Disallow: /


Sitemap: https://s3.amazonaws.com/<%= CHF::Env.lookup("sitemap_s3_bucket") %>/sitemap/sitemap.xml.gz

<% else %>
# non-production robots.txt

# let twitter scrape cards metadata for testing
User-agent: Twitterbot
Disallow:

User-agent: *
Disallow: /

<% end %>
